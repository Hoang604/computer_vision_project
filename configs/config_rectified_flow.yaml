# ===================================================================
#             Configuration for Rectified Flow Training
# ===================================================================
# This is the main configuration file for the train_rectified_flow.py script.
# Please fill in the appropriate paths and parameters before running.

# --- Training Mode & Device ---
mode: 'rectified' # Training mode: 'rectified' (for 1-RF) or 'reflow' (for 2-RF).
device: 'cuda:0'  # Device for training (e.g., 'cuda:0', 'cpu').
num_workers: 4    # Number of workers for the DataLoader.

# --- Data Paths ---
# Fill in the paths to the pre-processed data folders.
rectified_data_folder: './preprocessed_data/rrdb_data_train' # Data for Stage 1 (contains hr_original, lr, etc.).
reflow_data_folder: './preprocessed_data/reflow_data_train'   # Data for Stage 2 (generated by prepare_data_for_reflow.py).
val_data_folder: './preprocessed_data/rrdb_data_val'          # Validation data (same structure as the corresponding train folder).

# --- Step-Based Training Parameters ---
max_train_steps: 300000        # Total number of training steps to perform.
validate_every_n_steps: 2000   # Frequency to run validation and generate sample images.
save_every_n_steps: 2000       # Frequency to save the "_latest" checkpoint.

# --- Batch Size & Gradient Accumulation ---
batch_size: 32                 # Batch size per device.
val_batch_size: 32             # Batch size for validation.
accumulation_steps: 1          # Number of gradient accumulation steps (effective_batch_size = batch_size * accumulation_steps).

# --- Optimizer & Scheduler ---
learning_rate: 0.0001          # Initial learning rate for AdamW.
weight_decay: 0.0001           # Weight decay coefficient.
eta_min_lr: 0.000001           # Minimum learning rate for CosineAnnealingLR.

# --- Model Architecture ---
# UNet
unet_base_dim: 64              # Base channel dimension for the UNet.
unet_dim_mults: [1, 2, 4, 8]   # Channel multipliers for the UNet blocks.
use_attention: True            # Enable/disable attention mechanism in the UNet.

# RRDBNet (Context Extractor)
# A pre-trained RRDBNet weights file must be provided.
# The architectural parameters (nf, nb, gc) will be automatically inferred from this weights file.
rrdb_weights_path: './checkpoints/rrdb_g_best.pth'

# --- Logging & Resuming ---
# Provide a path to a checkpoint to resume training.
# The script will automatically load the state of the model, optimizer, scheduler, and step.
resume_checkpoint_path: '' # E.g., './checkpoints/rectified_flow/Rectified_20250911-125000/model_Rectified_20250911-125000_latest.pth'

# Directories for saving TensorBoard logs and checkpoints.
# If left empty, the script will create directories based on the mode and a timestamp.
log_dir: ''         # E.g., './logs_rectified_flow/Rectified_20250911-125000'
checkpoint_dir: ''  # E.g., './checkpoints/rectified_flow/Rectified_20250911-125000'
